"""
ì¶”ì¶œ ì„œë¹„ìŠ¤

PDFì—ì„œ í‘œë¥¼ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ë¡œì§ì„ ë‹´ë‹¹
"""

import asyncio
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path

from models.table_models import TableData, ExtractionLibrary
from core.pdf_processor.factory import PDFProcessorFactory
from services.ai_extraction_service import ai_extraction_service, AIExtractionRequest
from core.table_analyzer import get_table_structure_analyzer, TableStructureAnalysis

logger = logging.getLogger(__name__)


class ExtractionService:
    """ì¶”ì¶œ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì¶”ì¶œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.processor_factory = PDFProcessorFactory()
        self.table_analyzer = get_table_structure_analyzer()
        logger.info("ì¶”ì¶œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def extract_tables(
        self, 
        file_path: str, 
        library: str
    ) -> List[TableData]:
        """
        PDFì—ì„œ í‘œ ì¶”ì¶œ
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            library: ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ë¦„
            
        Returns:
            List[TableData]: ì¶”ì¶œëœ í‘œ ë°ì´í„° ëª©ë¡
        """
        try:
            logger.info(f"í‘œ ì¶”ì¶œ ì‹œì‘: {file_path}, ë¼ì´ë¸ŒëŸ¬ë¦¬: {library}")
            
            # íŒŒì¼ ê²½ë¡œë¥¼ Path ê°ì²´ë¡œ ë³€í™˜
            from pathlib import Path
            pdf_path = Path(file_path)
            
            # í”„ë¡œì„¸ì„œ ìƒì„±
            processor = self.processor_factory.create_processor(library)
            if not processor:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤: {library}")
            
            # í‘œ ì¶”ì¶œ ì‹¤í–‰
            tables = processor.extract_tables(pdf_path)
            
            # ì´ë¯¸ TableData ê°ì²´ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
            logger.info(f"í‘œ ì¶”ì¶œ ì™„ë£Œ: {len(tables)}ê°œ í‘œ ì¶”ì¶œ")
            return tables
            
        except Exception as e:
            logger.error(f"í‘œ ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
            raise
    
    async def extract_data_with_mappings(
        self, 
        file_path: str, 
        mappings: List[Dict[str, Any]],
        processor_type: str = "pdfplumber"
    ) -> Dict[str, Any]:
        """
        ë§¤í•‘ ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ (í…œí”Œë¦¿ ê¸°ë°˜ ì¶”ì¶œìš©)
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            mappings: í‚¤-ê°’ ë§¤í•‘ ì„¤ì •
            processor_type: PDF ì²˜ë¦¬ê¸° íƒ€ì…
            
        Returns:
            Dict[str, Any]: ì¶”ì¶œ ê²°ê³¼
        """
        import time
        from datetime import datetime
        
        start_time = time.time()
        
        try:
            logger.info(f"ë§¤í•‘ ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ ì‹œì‘: {file_path}")
            
            # 1. ë¨¼ì € í…Œì´ë¸” ì¶”ì¶œ
            tables = await self.extract_tables(file_path, processor_type)
            
            if not tables:
                logger.warning("ì¶”ì¶œëœ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤")
                return {
                    "extracted_data": [],
                    "processing_time": time.time() - start_time,
                    "extracted_at": datetime.now().isoformat()
                }
            
            # 2. ë§¤í•‘ì— ë”°ë¼ ë°ì´í„° ì¶”ì¶œ
            extracted_data = []
            
            for mapping in mappings:
                try:
                    # ë§¤í•‘ ì •ë³´ íŒŒì‹±
                    key = mapping.get("key")
                    key_label = mapping.get("key_label", key)
                    anchor_cell = mapping.get("anchor_cell", {})
                    value_cell = mapping.get("value_cell", {})
                    
                    if not anchor_cell or not value_cell:
                        logger.warning(f"ë§¤í•‘ ì •ë³´ê°€ ë¶ˆì™„ì „í•©ë‹ˆë‹¤: {key}")
                        continue
                    
                    # ì•µì»¤ ì…€ ì •ë³´
                    anchor_row = anchor_cell.get("row")
                    anchor_col = anchor_cell.get("col")
                    anchor_value = anchor_cell.get("value")
                    
                    # ê°’ ì…€ ì •ë³´
                    value_row = value_cell.get("row")
                    value_col = value_cell.get("col")
                    
                    # í…Œì´ë¸”ì—ì„œ í•´ë‹¹ ë°ì´í„° ì°¾ê¸°
                    extracted_value = None
                    confidence = 0.0
                    
                    for table in tables:
                        # TableData ëª¨ë¸ì˜ data ì†ì„± ì‚¬ìš©
                        table_data = table.rows if hasattr(table, 'rows') else []
                        
                        if (anchor_row < len(table_data) and 
                            anchor_col < len(table_data[anchor_row]) and
                            table_data[anchor_row][anchor_col] == anchor_value):
                            
                            # ì•µì»¤ë¥¼ ì°¾ì•˜ìœ¼ë©´ ê°’ ì¶”ì¶œ
                            if (value_row < len(table_data) and 
                                value_col < len(table_data[value_row])):
                                extracted_value = table_data[value_row][value_col]
                                confidence = 1.0
                                break
                    
                    if extracted_value is not None:
                        extracted_data.append({
                            "key": key,
                            "key_label": key_label,
                            "extracted_value": extracted_value,
                            "anchor_cell": anchor_cell,
                            "value_cell": value_cell,
                            "relative_position": mapping.get("relative_position", {}),
                            "confidence": confidence
                        })
                        logger.info(f"ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: {key} = {extracted_value}")
                    else:
                        logger.warning(f"ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {key} - ì•µì»¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        
                except Exception as e:
                    logger.error(f"ë§¤í•‘ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({mapping.get('key', 'unknown')}): {e}")
                    continue
            
            processing_time = time.time() - start_time
            
            logger.info(f"ë§¤í•‘ ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {len(extracted_data)}ê°œ í•­ëª©, {processing_time:.2f}ì´ˆ")
            
            return {
                "extracted_data": extracted_data,
                "processing_time": processing_time,
                "extracted_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ë§¤í•‘ ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
            raise
    
    async def quick_test(
        self, 
        file_path: str, 
        template_name: str,
        mappings: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ë¹ ë¥¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            template_name: í…œí”Œë¦¿ ì´ë¦„
            mappings: ë§¤í•‘ ì„¤ì •
            
        Returns:
            Dict[str, Any]: í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        try:
            logger.info(f"ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘: {file_path}")
            
            return {
                "success": True,
                "message": "ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ",
                "file_path": file_path,
                "config_valid": True
            }
            
        except Exception as e:
            logger.error(f"ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "message": f"ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}",
                "file_path": file_path,
                "config_valid": False
            }
    
    async def validate_mappings(
        self, 
        mappings: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ë§¤í•‘ ì„¤ì • ê²€ì¦
        
        Args:
            mappings: ê²€ì¦í•  ë§¤í•‘ ì„¤ì •
            
        Returns:
            Dict[str, Any]: ê²€ì¦ ê²°ê³¼
        """
        try:
            logger.info(f"ë§¤í•‘ ê²€ì¦ ì‹œì‘: {len(mappings)}ê°œ ë§¤í•‘")
            
            # ì„ì‹œ êµ¬í˜„
            return {
                "valid": True,
                "valid_count": len(mappings),
                "invalid_count": 0,
                "errors": []
            }
            
        except Exception as e:
            logger.error(f"ë§¤í•‘ ê²€ì¦ ì‹¤íŒ¨: {e}")
            raise

    async def recognize_keys(
        self, 
        file_path: str, 
        processor_type: str = "pdfplumber",
        save_debug: bool = False
    ) -> Dict[str, Any]:
        """
        í‚¤ ì¸ì‹ ìˆ˜í–‰
        PDFì—ì„œ ì•µì»¤ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ë¥¼ ìë™ ì¸ì‹í•©ë‹ˆë‹¤.
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            processor_type: PDF ì²˜ë¦¬ê¸° íƒ€ì…
            save_debug: ë””ë²„ê¹…ìš© íŒŒì¼ ì €ì¥ ì—¬ë¶€
            
        Returns:
            Dict[str, Any]: ì¸ì‹ëœ í‚¤ ì •ë³´
        """
        try:
            logger.info(f"í‚¤ ì¸ì‹ ì‹œì‘: {file_path}, í”„ë¡œì„¸ì„œ: {processor_type}")
            
            # 1ë‹¨ê³„: PDFì—ì„œ í‘œ ì¶”ì¶œ
            tables = await self.extract_tables(file_path, processor_type)
            
            # 2ë‹¨ê³„: í…Œì´ë¸” êµ¬ì¡° ë¶„ì„
            table_analyses = []
            for table in tables:
                if table.rows:
                    analysis = await self.table_analyzer.analyze_table_structure(table, use_ai=True)
                    table_analyses.append(analysis)
                    logger.info(f"í…Œì´ë¸” êµ¬ì¡° ë¶„ì„ ì™„ë£Œ - í˜ì´ì§€ {table.page_number}: {analysis.orientation.value}, ë³µì¡ë„: {analysis.complexity.value}")
            
            # 3ë‹¨ê³„: í‚¤ ì¸ì‹ ë¡œì§ ìˆ˜í–‰
            recognized_keys = []
            pattern_suggestions = []  # ìƒˆ íŒ¨í„´ ì œì•ˆ ëª©ë¡
            new_key_suggestions = []  # ìƒˆ í‚¤ ì œì•ˆ ëª©ë¡
            
            # í‚¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ ë° ì¶”ì¶œ ì„¤ì • ë¡œë“œ
            key_mapping_db = await self._load_key_mapping_database()
            extraction_settings = await self._load_extraction_settings()
            
            logger.info(f"í‚¤ ë§¤í•‘ DB ë¡œë“œ ì™„ë£Œ: {len(key_mapping_db)}ê°œ í‚¤")
            logger.info(f"ì¶”ì¶œ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {len(extraction_settings)}ê°œ ì„¹ì…˜")
            
            for table_idx, table in enumerate(tables):
                if not table.rows:
                    continue
                
                # í•´ë‹¹ í…Œì´ë¸”ì˜ êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì°¾ê¸°
                table_analysis = None
                for analysis in table_analyses:
                    if analysis.page_number == table.page_number:
                        table_analysis = analysis
                        break
                
                # êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ í‚¤ ì¸ì‹
                if table_analysis:
                    logger.info(f"êµ¬ì¡° ê¸°ë°˜ í‚¤ ì¸ì‹ - í˜ì´ì§€ {table.page_number}: {table_analysis.extraction_strategy}")
                else:
                    logger.warning(f"í…Œì´ë¸” êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì—†ìŒ - í˜ì´ì§€ {table.page_number}, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©")
                    
                logger.info(f"í‘œ ë¶„ì„ ì¤‘: í˜ì´ì§€ {table.page_number}, {len(table.rows)}í–‰ x {table.col_count}ì—´")
                
                # í—¤ë”ì™€ ë°ì´í„° í–‰ì„ ëª¨ë‘ í¬í•¨í•˜ì—¬ ë¶„ì„
                all_rows = []
                if table.headers:
                    all_rows.append(table.headers)
                all_rows.extend(table.rows)
                
                for row_idx, row in enumerate(all_rows):
                    for col_idx, cell_value in enumerate(row):
                        if not cell_value or not isinstance(cell_value, str):
                            continue
                            
                        cell_text = cell_value.strip()
                        
                        # ê° í‚¤ì— ëŒ€í•´ ë§¤ì¹­ í™•ì¸ (ì¹´í…Œê³ ë¦¬ë³„ ìˆœíšŒ)
                        matched = False
                        for category_name, category_data in key_mapping_db.items():
                            # ì„¤ì • ì„¹ì…˜ ê±´ë„ˆë›°ê¸°
                            if category_name.startswith("_"):
                                continue
                            for key_name, key_data in category_data.items():
                                # ìƒˆë¡œìš´ ë³µí•© êµ¬ì¡°ì¸ì§€ êµ¬ ë°°ì—´ êµ¬ì¡°ì¸ì§€ í™•ì¸
                                patterns = []
                                if isinstance(key_data, dict) and "patterns" in key_data:
                                    patterns = key_data["patterns"]  # ìƒˆ êµ¬ì¡°
                                elif isinstance(key_data, list):
                                    patterns = key_data  # êµ¬ êµ¬ì¡°
                                
                                for pattern in patterns:
                                    # ì •í™•í•œ ë§¤ì¹­ ìš°ì„  (ê³µë°± ì œê±° í›„ ë¹„êµ)
                                    cell_clean = cell_text.strip().lower()
                                    pattern_clean = pattern.strip().lower()
                                    
                                    # ì„¤ì •ì—ì„œ ì‹ ë¢°ë„ ì„ê³„ê°’ ë¡œë“œ
                                    thresholds = extraction_settings.get("confidence_thresholds", {})
                                    text_settings = extraction_settings.get("text_processing", {})
                                    
                                    min_pattern_length = text_settings.get("minimum_pattern_length", 3)
                                    
                                    # 1. ì •í™•í•œ ë§¤ì¹­ (ë†’ì€ ì‹ ë¢°ë„)
                                    if cell_clean == pattern_clean:
                                        confidence = thresholds.get("exact_match", 0.95)
                                        match_type = "exact"
                                    # 2. ì‹œì‘ ë¶€ë¶„ ë§¤ì¹­ (ì¤‘ê°„ ì‹ ë¢°ë„)
                                    elif cell_clean.startswith(pattern_clean):
                                        confidence = thresholds.get("prefix_match", 0.8)
                                        match_type = "prefix"
                                    # 3. ë ë¶€ë¶„ ë§¤ì¹­ (ì¤‘ê°„ ì‹ ë¢°ë„)
                                    elif cell_clean.endswith(pattern_clean):
                                        confidence = thresholds.get("suffix_match", 0.7)
                                        match_type = "suffix"
                                    # 4. ë¶€ë¶„ ë§¤ì¹­ (ë‚®ì€ ì‹ ë¢°ë„, ë‹¨ì–´ ê²½ê³„ ê³ ë ¤)
                                    elif pattern_clean in cell_clean and len(pattern_clean) >= min_pattern_length:
                                        # ë‹¨ì–´ ê²½ê³„ í™•ì¸ (ê³µë°±, ê´„í˜¸, íŠ¹ìˆ˜ë¬¸ì ë“±)
                                        import re
                                        word_boundary_pattern = r'\b' + re.escape(pattern_clean) + r'\b'
                                        if re.search(word_boundary_pattern, cell_clean):
                                            confidence = thresholds.get("word_boundary_match", 0.6)
                                            match_type = "word_boundary"
                                        else:
                                            confidence = thresholds.get("partial_match", 0.4)
                                            match_type = "partial"
                                    else:
                                        continue
                                
                                    # ì‹ ë¢°ë„ ì„ê³„ê°’ í™•ì¸ (ì„¤ì •ê°’ ì´ìƒë§Œ í—ˆìš©)
                                    minimum_acceptance = thresholds.get("minimum_acceptance", 0.4)
                                    if confidence >= minimum_acceptance:
                                        logger.info(f"í‚¤ ë§¤ì¹­ ë°œê²¬: '{pattern}' -> '{key_name}' (ì…€: '{cell_text}', íƒ€ì…: {match_type}, ì‹ ë¢°ë„: {confidence})")
                                    
                                    # ì¸ì‹ëœ í‚¤ ì •ë³´ ìƒì„±
                                    import time
                                    unique_id = f"{key_name}_{table.page_number}_{row_idx}_{col_idx}_{int(time.time() * 1000000)}_{len(recognized_keys)}"
                                    # ë³µí•© í•„ë“œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                                    expected_fields = await self._get_expected_fields_for_key(key_name, key_mapping_db)
                                    
                                    # ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
                                    key_category = "unknown"
                                    for cat_name, cat_data in key_mapping_db.items():
                                        if cat_name.startswith("_"):  # ì„¤ì • ì„¹ì…˜ ìŠ¤í‚µ
                                            continue
                                        if key_name in cat_data:
                                            key_category = cat_name
                                            break
                                    
                                    recognized_key = {
                                        "id": unique_id,  # ê³ ìœ  ID ì¶”ê°€
                                        "key": key_name,
                                        "key_name": key_name,  # ì‹œë®¬ë ˆì´ì…˜ í˜¸í™˜ì„±
                                        "key_label": key_name,  # í‘œì‹œìš© ë¼ë²¨
                                        "category": key_category,  # ì¹´í…Œê³ ë¦¬ ì •ë³´
                                        "anchor_cell": {
                                            "text": cell_text,
                                            "page_number": table.page_number,
                                            "row": row_idx,
                                            "col": col_idx
                                        },
                                        # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•„ë“œ ìœ ì§€
                                        "value_cell": {
                                            "page_number": table.page_number,
                                            "row": row_idx + 1,  # ë‹¤ìŒ í–‰ ì œì•ˆ
                                            "col": col_idx
                                        },
                                        "relative_position": "next_row",
                                        "table_id": f"table_{table.page_number}_{row_idx}",
                                        "confidence": confidence,
                                        "match_type": match_type,
                                        
                                        # ìƒˆë¡œìš´ ë³µí•© í•„ë“œ ì§€ì›
                                        "is_complex": bool(expected_fields),
                                        "expected_fields": expected_fields or {},
                                        "detected_fields": await self._detect_complex_fields(
                                            table, row_idx, col_idx, expected_fields
                                        ) if expected_fields else {}
                                    }
                                    
                                    # í‚¤ë³„ ë‹¤ì¤‘ ìœ„ì¹˜ ê´€ë¦¬ (ì—¬ëŸ¬ ìœ„ì¹˜ ë°œê²¬ ì‹œ ëª¨ë‘ í‘œì‹œ)
                                    existing_key = next((rk for rk in recognized_keys if rk["key"] == key_name), None)
                                    if existing_key:
                                        # ê¸°ì¡´ í‚¤ì— alternative_locations ì¶”ê°€
                                        if "alternative_locations" not in existing_key:
                                            existing_key["alternative_locations"] = []
                                        
                                        # ìƒˆë¡œìš´ ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
                                        alternative_location = {
                                            "anchor_cell": {
                                                "text": cell_text,
                                                "page_number": table.page_number,
                                                "row": row_idx,
                                                "col": col_idx
                                            },
                                            "confidence": confidence,
                                            "match_type": match_type
                                        }
                                        existing_key["alternative_locations"].append(alternative_location)
                                        existing_key["multiple_found"] = True
                                        existing_key["total_locations"] = 1 + len(existing_key["alternative_locations"])
                                        logger.info(f"í‚¤ ì¤‘ë³µ ìœ„ì¹˜ ì¶”ê°€: {key_name} (ì´ {existing_key['total_locations']}ê°œ ìœ„ì¹˜, ìƒˆ ìœ„ì¹˜: í˜ì´ì§€ {table.page_number}, í–‰ {row_idx}, ì—´ {col_idx}, ì‹ ë¢°ë„: {confidence})")
                                    else:
                                        # ìƒˆë¡œìš´ í‚¤ ì¶”ê°€ (ë‹¤ì¤‘ ìœ„ì¹˜ ì§€ì› í•„ë“œ ì´ˆê¸°í™”)
                                        recognized_key["multiple_found"] = False
                                        recognized_key["alternative_locations"] = []
                                        recognized_key["total_locations"] = 1
                                        recognized_key["selected_location"] = "primary"  # ê¸°ë³¸ ì„ íƒ: primary ìœ„ì¹˜
                                        recognized_keys.append(recognized_key)
                                        logger.info(f"ìƒˆ í‚¤ ì¶”ê°€: {key_name} (í˜ì´ì§€ {table.page_number}, í–‰ {row_idx}, ì—´ {col_idx}, ì‹ ë¢°ë„: {confidence})")
                                    matched = True
                                    break
                            if matched:
                                break
                        
                        # ë§¤ì¹­ë˜ì§€ ì•Šì€ ì…€ ì¤‘ì—ì„œ ìƒˆë¡œìš´ íŒ¨í„´ ì œì•ˆ í™•ì¸
                        text_settings = extraction_settings.get("text_processing", {})
                        max_cell_length = text_settings.get("maximum_cell_length", 20)
                        
                        if not matched and len(cell_text) > 1 and len(cell_text) < max_cell_length:
                            # ìˆ«ìê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ì´ê³ , ë„ˆë¬´ ê¸¸ì§€ ì•Šì€ ê²½ìš°
                            if not cell_text.isdigit() and not any(char.isdigit() for char in cell_text):
                                # ê¸°ì¡´ í‚¤ì™€ ìœ ì‚¬ë„ ë¶„ì„í•˜ì—¬ íŒ¨í„´ ì œì•ˆ
                                suggested_key = await self._analyze_pattern_similarity(cell_text, key_mapping_db)
                                if suggested_key:
                                    logger.info(f"ìƒˆ íŒ¨í„´ ì œì•ˆ: '{cell_text}' -> '{suggested_key}' í‚¤ì— ì¶”ê°€ ì œì•ˆ")
                                    # ì œì•ˆ ëª©ë¡ì— ì¶”ê°€ (ë‚˜ì¤‘ì— ì‘ë‹µì— í¬í•¨)
                                    pattern_suggestions.append({
                                        "anchor_text": cell_text,
                                        "suggested_key": suggested_key,
                                        "location": {
                                            "page_number": table.page_number,
                                            "row": row_idx,
                                            "col": col_idx
                                        },
                                        "confidence": 0.6  # ì œì•ˆ ì‹ ë¢°ë„
                                    })
                                else:
                                    # ì™„ì „íˆ ìƒˆë¡œìš´ í‚¤ í›„ë³´ ê°ì§€
                                    logger.info(f"ìƒˆ í‚¤ í›„ë³´ ë°œê²¬: '{cell_text}' (í˜ì´ì§€ {table.page_number}, í–‰ {row_idx}, ì—´ {col_idx})")
                                    
                                    # ìƒˆë¡œìš´ í‚¤ ì¹´í…Œê³ ë¦¬ ì¶”ì •
                                    suggested_category = await self._estimate_new_key_category(cell_text)
                                    
                                    # ìƒˆ í‚¤ ì œì•ˆ ëª©ë¡ì— ì¶”ê°€
                                    new_key_suggestions.append({
                                        "key_name": cell_text,
                                        "suggested_category": suggested_category,
                                        "location": {
                                            "page_number": table.page_number,
                                            "row": row_idx,
                                            "col": col_idx
                                        },
                                        "confidence": 0.5,  # ìƒˆ í‚¤ ê¸°ë³¸ ì‹ ë¢°ë„
                                        "patterns": [cell_text]  # ê¸°ë³¸ íŒ¨í„´
                                    })
                                    
                                    # ê¸°ì¡´ ë°©ì‹ë„ ìœ ì§€ (ë°±ì—…)
                                    await self._save_new_key(cell_text, cell_text, suggested_category)
            
            logger.info(f"í‚¤ ì¸ì‹ ì™„ë£Œ: {len(recognized_keys)}ê°œ í‚¤ ë°œê²¬")
            
            # ë””ë²„ê¹…ìš© ì €ì¥ (ì„ íƒì )
            debug_info = None
            if save_debug:
                debug_info = await self._save_debug_data(file_path, processor_type, {
                    "recognized_keys": recognized_keys,
                    "total_found": len(recognized_keys),
                    "pages_analyzed": len(set(table.page_number for table in tables)),
                    "tables_analyzed": len(tables),
                    "pattern_suggestions": pattern_suggestions,
                    "suggestions_count": len(pattern_suggestions),
                    "new_key_suggestions": new_key_suggestions,
                    "new_keys_count": len(new_key_suggestions),
                    "table_structure_analyses": [
                        {
                            "table_id": analysis.table_id,
                            "page_number": analysis.page_number,
                            "orientation": analysis.orientation.value,
                            "complexity": analysis.complexity.value,
                            "headers_count": len(analysis.headers),
                            "data_start": {"row": analysis.data_start_row, "col": analysis.data_start_col},
                            "confidence": analysis.confidence,
                            "extraction_strategy": analysis.extraction_strategy,
                            "analysis_notes": analysis.analysis_notes
                        }
                        for analysis in table_analyses
                    ],
                    "raw_tables": tables
                })
            
            result = {
                "recognized_keys": recognized_keys,
                "total_found": len(recognized_keys),
                "pages_analyzed": len(set(table.page_number for table in tables)),
                "tables_analyzed": len(tables),
                "pattern_suggestions": pattern_suggestions,
                "suggestions_count": len(pattern_suggestions),
                "new_key_suggestions": new_key_suggestions,
                "new_keys_count": len(new_key_suggestions)
            }
            
            # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€ (ì €ì¥í–ˆì„ ë•Œë§Œ)
            if debug_info:
                result["debug_saved"] = debug_info
                logger.info(f"ğŸ› ë””ë²„ê¹… ë°ì´í„° ì €ì¥ë¨: {debug_info['file_path']}")
            
            return result
            
        except Exception as e:
            logger.error(f"í‚¤ ì¸ì‹ ì‹¤íŒ¨: {str(e)}")
            return {
                "recognized_keys": [],
                "total_found": 0,
                "pages_analyzed": 0,
                "tables_analyzed": 0,
                "error": str(e)
            }
    
    async def _load_key_mapping_database(self) -> Dict:
        """
        í‚¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ (ì¹´í…Œê³ ë¦¬ êµ¬ì¡° ìœ ì§€)
        JSON íŒŒì¼ì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ë°˜í™˜
        
        Returns:
            Dict: í‚¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤ (ì¹´í…Œê³ ë¦¬ êµ¬ì¡° ìœ ì§€)
        """
        try:
            import json
            import os
            
            # JSON íŒŒì¼ ê²½ë¡œ
            json_file_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'key_mapping_database.json')
            
            if os.path.exists(json_file_path):
                with open(json_file_path, 'r', encoding='utf-8') as f:
                    categorized_db = json.load(f)
                    
                    logger.info(f"í‚¤ ë§¤í•‘ DB ë¡œë“œ ì™„ë£Œ: {len(categorized_db)}ê°œ ì¹´í…Œê³ ë¦¬")
                    logger.info(f"ì¹´í…Œê³ ë¦¬: {list(categorized_db.keys())}")
                    
                    # ì¹´í…Œê³ ë¦¬ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ì—¬ ë°˜í™˜ (ë³µí•© í•„ë“œ ì§€ì›)
                    return categorized_db
            else:
                logger.warning(f"í‚¤ ë§¤í•‘ DB JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {json_file_path}")
                # ê¸°ë³¸ê°’ ë°˜í™˜ (ì¹´í…Œê³ ë¦¬ êµ¬ì¡°)
                return {
                    "basic": {
                        "í‚¤": ["í‚¤", "key", "í•­ëª©", "êµ¬ë¶„"],
                        "ì‹ ì¥": ["ì‹ ì¥", "í‚¤", "height"],
                        "ì²´ì¤‘": ["ì²´ì¤‘", "ëª¸ë¬´ê²Œ", "weight"],
                            "í˜ˆì••": ["í˜ˆì••", "blood pressure", "BP"],
                        "í˜ˆë‹¹": ["í˜ˆë‹¹", "glucose", "ë‹¹ë‡¨"],
                        "ì„±ëª…": ["ì„±ëª…", "ì´ë¦„", "name"],
                        "ë‚˜ì´": ["ë‚˜ì´", "age", "ì—°ë ¹"],
                        "ì„±ë³„": ["ì„±ë³„", "gender", "sex"]
                    }
                }
        except Exception as e:
            logger.error(f"í‚¤ ë§¤í•‘ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _analyze_pattern_similarity(self, anchor_text: str, key_mapping_db: Dict[str, List[str]]) -> Optional[str]:
        """
        ì•µì»¤ í…ìŠ¤íŠ¸ì™€ ê¸°ì¡´ í‚¤ íŒ¨í„´ë“¤ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ í‚¤ë¥¼ ì œì•ˆ
        
        Args:
            anchor_text: ë¶„ì„í•  ì•µì»¤ í…ìŠ¤íŠ¸
            key_mapping_db: í‚¤ ë§¤í•‘ ë°ì´í„°ë² ì´ìŠ¤
            
        Returns:
            str: ì œì•ˆí•  í‚¤ ì´ë¦„ (ìœ ì‚¬ë„ê°€ ë†’ì€ ê²½ìš°), None (ìœ ì‚¬ë„ê°€ ë‚®ì€ ê²½ìš°)
        """
        try:
            import difflib
            
            # ì„¤ì •ì—ì„œ ìœ ì‚¬ë„ ì„ê³„ê°’ ë¡œë“œ
            extraction_settings = await self._load_extraction_settings()
            thresholds = extraction_settings.get("confidence_thresholds", {})
            text_settings = extraction_settings.get("text_processing", {})
            
            max_similarity = 0.0
            suggested_key = None
            threshold = thresholds.get("similarity_threshold", 0.4)  # ìœ ì‚¬ë„ ì„ê³„ê°’
            
            anchor_lower = anchor_text.lower().strip()
            
            # ê° í‚¤ì˜ íŒ¨í„´ë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
            for key_name, patterns in key_mapping_db.items():
                for pattern in patterns:
                    pattern_lower = pattern.lower().strip()
                    
                    # ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚° (SequenceMatcher ì‚¬ìš©)
                    similarity = difflib.SequenceMatcher(None, anchor_lower, pattern_lower).ratio()
                    
                    # ë¶€ë¶„ ë¬¸ìì—´ í¬í•¨ ì—¬ë¶€ë„ ê³ ë ¤ (ê°€ì¤‘ì¹˜ ì ìš©)
                    partial_bonus = text_settings.get("partial_match_bonus", 0.3)
                    korean_bonus = text_settings.get("korean_bonus", 0.2)
                    korean_threshold = text_settings.get("korean_similarity_threshold", 0.3)
                    
                    if anchor_lower in pattern_lower or pattern_lower in anchor_lower:
                        similarity += partial_bonus  # ë¶€ë¶„ ë§¤ì¹­ ë³´ë„ˆìŠ¤
                    
                    # í•œê¸€ ìëª¨ìŒ ìœ ì‚¬ë„ë„ ê³ ë ¤ (ì˜ˆ: "ê¸°ë†’ì´" vs "í‚¤")
                    if self._check_korean_similarity(anchor_lower, pattern_lower, korean_threshold):
                        similarity += korean_bonus  # í•œê¸€ ìœ ì‚¬ë„ ë³´ë„ˆìŠ¤
                    
                    if similarity > max_similarity:
                        max_similarity = similarity
                        suggested_key = key_name
            
            # ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ì—ë§Œ ì œì•ˆ
            if max_similarity >= threshold:
                logger.info(f"íŒ¨í„´ ìœ ì‚¬ë„ ë¶„ì„: '{anchor_text}' -> '{suggested_key}' (ìœ ì‚¬ë„: {max_similarity:.2f})")
                return suggested_key
            else:
                logger.info(f"íŒ¨í„´ ìœ ì‚¬ë„ ë¶„ì„: '{anchor_text}' - ì í•©í•œ í‚¤ ì—†ìŒ (ìµœëŒ€ ìœ ì‚¬ë„: {max_similarity:.2f})")
                return None
                
        except Exception as e:
            logger.error(f"íŒ¨í„´ ìœ ì‚¬ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _check_korean_similarity(self, text1: str, text2: str, threshold: float = 0.3) -> bool:
        """í•œê¸€ í…ìŠ¤íŠ¸ì˜ ìëª¨ìŒ ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ì‚¬"""
        try:
            # ê°„ë‹¨í•œ í•œê¸€ ìœ ì‚¬ë„ ê²€ì‚¬ (ì´ˆì„±, ì¤‘ì„±, ì¢…ì„± ë¹„êµ)
            # ì˜ˆ: "ê¸°" vs "í‚¤" (ì´ˆì„±ì´ ê°™ìŒ)
            common_chars = set(text1) & set(text2)
            similarity = len(common_chars) / max(len(set(text1)), len(set(text2)))
            return similarity > threshold
        except:
            return False
    
    async def _estimate_new_key_category(self, key_text: str) -> str:
        """ìƒˆë¡œìš´ í‚¤ì˜ ì¹´í…Œê³ ë¦¬ ì¶”ì • (ë™ì  í‚¤ì›Œë“œ ë¡œë“œ)"""
        try:
            key_lower = key_text.lower().strip()
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í‚¤ì›Œë“œë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œ
            classification_keywords = await self._load_category_classification_keywords()
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¹­ í™•ì¸ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
            category_priority = ["personal", "temporal", "opinion", "basic", "cancer", "comprehensive", "special"]
            
            for category in category_priority:
                if category in classification_keywords:
                    keywords = classification_keywords[category]
                    for keyword in keywords:
                        if keyword.lower() in key_lower:
                            logger.debug(f"í‚¤ '{key_text}' -> ì¹´í…Œê³ ë¦¬ '{category}' (í‚¤ì›Œë“œ: '{keyword}')")
                            return category
            
            # ê¸°ë³¸ê°’ì€ specialë¡œ ì„¤ì •
            logger.debug(f"í‚¤ '{key_text}' -> ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ 'special' (ë§¤ì¹­ í‚¤ì›Œë“œ ì—†ìŒ)")
            return "special"
            
        except Exception as e:
            logger.error(f"í‚¤ ì¹´í…Œê³ ë¦¬ ì¶”ì • ì‹¤íŒ¨: {e}")
            return "special"

    async def _load_extraction_settings(self) -> Dict[str, Any]:
        """ì¶”ì¶œ ì„¤ì •ì„ key_mapping_database.jsonì—ì„œ ë¡œë“œ"""
        try:
            import json
            import os
            
            json_path = os.path.join(os.path.dirname(__file__), "..", "data", "key_mapping_database.json")
            
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # _extraction_settings ì„¹ì…˜ ì¶”ì¶œ
            extraction_settings = data.get('_extraction_settings', {})
            
            if not extraction_settings:
                logger.warning("ì¶”ì¶œ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
                # ê¸°ë³¸ ì„¤ì • (ë°±ì—…ìš©)
                extraction_settings = {
                    "confidence_thresholds": {
                        "exact_match": 0.95,
                        "prefix_match": 0.8,
                        "suffix_match": 0.7,
                        "word_boundary_match": 0.6,
                        "partial_match": 0.4,
                        "minimum_acceptance": 0.4,
                        "ai_analysis_minimum": 0.7,
                        "similarity_threshold": 0.4
                    },
                    "field_detection_keywords": {
                        "result": ["ê²€ì‚¬ê²°ê³¼", "ê²°ê³¼", "ì¸¡ì •ê°’", "ê°’"],
                        "normal_range": ["ì •ìƒì¹˜", "ê¸°ì¤€ì¹˜", "ì •ìƒë²”ìœ„"],
                        "judgment": ["íŒì •", "ì†Œê²¬", "ê²°ê³¼"],
                        "opinion": ["ì†Œê²¬", "ì˜ê²¬", "ë¹„ê³ "]
                    }
                }
            
            logger.debug(f"ì¶”ì¶œ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {len(extraction_settings)}ê°œ ì„¹ì…˜")
            return extraction_settings
            
        except Exception as e:
            logger.error(f"ì¶”ì¶œ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ê¸°ë³¸ ì„¤ì • ë°˜í™˜
            return {
                "confidence_thresholds": {
                    "minimum_acceptance": 0.4,
                    "similarity_threshold": 0.4
                },
                "field_detection_keywords": {
                    "result": ["ê²°ê³¼"],
                    "normal_range": ["ì •ìƒì¹˜"]
                }
            }

    async def _load_category_classification_keywords(self) -> Dict[str, List[str]]:
        """ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í‚¤ì›Œë“œë¥¼ key_mapping_database.jsonì—ì„œ ë¡œë“œ"""
        try:
            import json
            import os
            
            json_path = os.path.join(os.path.dirname(__file__), "..", "data", "key_mapping_database.json")
            
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # _category_classification_keywords ì„¹ì…˜ ì¶”ì¶œ
            classification_keywords = data.get('_category_classification_keywords', {})
            
            if not classification_keywords:
                logger.warning("ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í‚¤ì›Œë“œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
                # ê¸°ë³¸ í‚¤ì›Œë“œ (ë°±ì—…ìš©)
                classification_keywords = {
                    "personal": ["ì´ë¦„", "ì„±ëª…", "ë‚˜ì´", "ì—°ë ¹", "ì„±ë³„"],
                    "temporal": ["ì´ì „", "ê³¼ê±°", "ì‘ë…„", "ì „ë…„", "ë…„ë„", "ê²°ê³¼"],
                    "basic": ["ì‹ ì¥", "í‚¤", "ì²´ì¤‘", "í˜ˆì••", "í˜ˆë‹¹"],
                    "special": ["ê²€ì‚¬", "ì†Œê²¬", "íŒì •", "ë¬¸ì§„", "ì´¬ì˜"],
                    "opinion": ["ì†Œê²¬", "ì˜ê²¬", "íŒì •", "ì§„ë‹¨"],
                    "cancer": ["ì•”", "ì¢…ì–‘", "ì•…ì„±", "ì–‘ì„±"],
                    "comprehensive": ["ì¢…í•©", "ì „ì²´", "í†µí•©"]
                }
            
            logger.debug(f"ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í‚¤ì›Œë“œ ë¡œë“œ ì™„ë£Œ: {len(classification_keywords)}ê°œ ì¹´í…Œê³ ë¦¬")
            return classification_keywords
            
        except Exception as e:
            logger.error(f"ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í‚¤ì›Œë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
            return {
                "personal": ["ì´ë¦„", "ì„±ëª…"],
                "basic": ["ì‹ ì¥", "ì²´ì¤‘"],
                "special": ["ê²€ì‚¬", "ì†Œê²¬"]
            }

    async def _get_expected_fields_for_key(self, key_name: str, key_mapping_db: Dict) -> Dict:
        """í‚¤ì— ëŒ€í•œ ì˜ˆìƒ í•„ë“œ êµ¬ì¡° ì¡°íšŒ"""
        try:
            # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ í‚¤ ì°¾ê¸°
            for category_name, category_data in key_mapping_db.items():
                if key_name in category_data:
                    key_data = category_data[key_name]
                    # ìƒˆë¡œìš´ ë³µí•© êµ¬ì¡°ì¸ì§€ í™•ì¸
                    if isinstance(key_data, dict) and "expected_fields" in key_data:
                        return key_data["expected_fields"]
            
            return {}
        except Exception as e:
            logger.error(f"ì˜ˆìƒ í•„ë“œ ì¡°íšŒ ì‹¤íŒ¨ ({key_name}): {e}")
            return {}

    async def _detect_complex_fields(self, table, anchor_row: int, anchor_col: int, expected_fields: Dict) -> Dict:
        """í…Œì´ë¸”ì—ì„œ ë³µí•© í•„ë“œ ìœ„ì¹˜ ê°ì§€ (AI í—¤ë” ë¶„ì„ í™œìš©)"""
        detected_fields = {}
        
        try:
            if not expected_fields or not table.rows:
                return detected_fields
                
            # í—¤ë” í–‰ì´ ìˆëŠ”ì§€ í™•ì¸
            header_row_idx = 0
            headers = []
            if len(table.rows) > 0:
                headers = table.rows[0] if table.rows[0] else []
            
            # AI í—¤ë” ë¶„ì„ ì‹œë„ (ì„ íƒì )
            ai_column_mappings = await self._try_ai_header_analysis(table, headers)
            
            # ê° ì˜ˆìƒ í•„ë“œì— ëŒ€í•´ ìœ„ì¹˜ ê°ì§€
            for field_type, field_info in expected_fields.items():
                try:
                    # AI ë¶„ì„ ê²°ê³¼ ìš°ì„  ì‚¬ìš©
                    field_col = None
                    if ai_column_mappings:
                        field_col = self._find_column_by_ai_mapping(ai_column_mappings, field_type)
                    
                    # AI ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                    if field_col is None:
                        field_col = await self._find_field_column(
                            headers, field_type, field_info, anchor_col
                        )
                    
                    if field_col is not None:
                        # ê°’ ì¶”ì¶œ ì‹œë„
                        field_value = await self._extract_field_value(
                            table, anchor_row, field_col, field_info
                        )
                        
                        # ì„¤ì •ì—ì„œ ê¸°ë³¸ í•„ë“œ ì‹ ë¢°ë„ ë¡œë“œ
                        extraction_settings = await self._load_extraction_settings()
                        ai_settings = extraction_settings.get("ai_analysis", {})
                        default_confidence = ai_settings.get("default_field_confidence", 0.8)
                        
                        detected_fields[field_type] = {
                            "col_offset": field_col - anchor_col,
                            "col": field_col,
                            "row": anchor_row,
                            "value": field_value,
                            "confidence": default_confidence,
                            "data_type": field_info.get("data_type", "text"),
                            "is_required": field_info.get("is_required", False)
                        }
                        
                        logger.debug(f"í•„ë“œ ê°ì§€: {field_type} -> ì—´ {field_col}, ê°’: {field_value}")
                        
                except Exception as field_error:
                    logger.error(f"í•„ë“œ ê°ì§€ ì‹¤íŒ¨ ({field_type}): {field_error}")
                    continue
                    
        except Exception as e:
            logger.error(f"ë³µí•© í•„ë“œ ê°ì§€ ì‹¤íŒ¨: {e}")
            
        return detected_fields

    async def _find_field_column(self, headers: List[str], field_type: str, field_info: Dict, anchor_col: int) -> Optional[int]:
        """í•„ë“œ íƒ€ì…ì— ë§ëŠ” ì—´ ì°¾ê¸° (ì„¤ì • ê¸°ë°˜)"""
        try:
            # ì„¤ì •ì—ì„œ í•„ë“œ ê°ì§€ í‚¤ì›Œë“œ ë¡œë“œ
            extraction_settings = await self._load_extraction_settings()
            field_keywords = extraction_settings.get("field_detection_keywords", {})
            
            keywords = field_keywords.get(field_type, [])
            
            # í—¤ë”ì—ì„œ í‚¤ì›Œë“œ ë§¤ì¹­
            for col_idx, header in enumerate(headers):
                if header and isinstance(header, str):
                    header_lower = header.lower().strip()
                    for keyword in keywords:
                        if keyword in header_lower:
                            return col_idx
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¶”ì •
            priority = field_info.get("priority", 999)
            estimated_col = anchor_col + priority
            
            if 0 <= estimated_col < len(headers):
                return estimated_col
                
        except Exception as e:
            logger.error(f"í•„ë“œ ì—´ ì°¾ê¸° ì‹¤íŒ¨ ({field_type}): {e}")
            
        return None

    async def _extract_field_value(self, table, row: int, col: int, field_info: Dict) -> str:
        """í•„ë“œ ê°’ ì¶”ì¶œ"""
        try:
            if (row < len(table.rows) and col < len(table.rows[row]) and 
                table.rows[row] and table.rows[row][col]):
                return str(table.rows[row][col]).strip()
        except Exception as e:
            logger.error(f"í•„ë“œ ê°’ ì¶”ì¶œ ì‹¤íŒ¨ (í–‰:{row}, ì—´:{col}): {e}")
            
        return ""

    async def _try_ai_header_analysis(self, table, headers: List[str]) -> Dict[int, str]:
        """AI í—¤ë” ë¶„ì„ ì‹œë„ (ì„ íƒì )"""
        try:
            # AI ì„œë¹„ìŠ¤ ì„í¬íŠ¸ ë° ì´ˆê¸°í™”
            from services.ai_extraction_service import AIExtractionService
            
            ai_service = AIExtractionService()
            if not ai_service.is_available():
                logger.debug("AI ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©")
                return {}
            
            # ì„¤ì •ì—ì„œ AI ë¶„ì„ ì„¤ì • ë¡œë“œ
            extraction_settings = await self._load_extraction_settings()
            ai_settings = extraction_settings.get("ai_analysis", {})
            max_rows = ai_settings.get("max_table_rows_for_analysis", 5)
            
            # í…Œì´ë¸” ë°ì´í„° êµ¬ì„±
            table_data = []
            if headers:
                table_data.append(headers)
            if table.rows:
                table_data.extend(table.rows[:max_rows-1])  # í—¤ë” + ë°ì´í„° í–‰ë“¤
                
            # AI í—¤ë” ë¶„ì„ ì‹¤í–‰
            result = await ai_service.analyze_table_headers(table_data, "ê²€ì§„í•­ëª©")
            
            ai_confidence_threshold = ai_settings.get("header_analysis_confidence_threshold", 0.7)
            if result and result.confidence >= ai_confidence_threshold:
                logger.info(f"AI í—¤ë” ë¶„ì„ ì„±ê³µ: ì‹ ë¢°ë„ {result.confidence:.2f}")
                logger.info(f"í…Œì´ë¸” êµ¬ì¡°: {result.table_structure}, í—¤ë” ë°©í–¥: {result.header_orientation}")
                logger.info(f"ê°ì§€ëœ í•„ë“œ: {result.detected_fields}")
                logger.info(f"êµ¬ì¡° ë¶„ì„: {result.table_analysis}")
                
                # ë¬¸ìì—´ í‚¤ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
                column_mappings = {}
                for col_str, field_type in result.column_mappings.items():
                    try:
                        col_int = int(col_str)
                        column_mappings[col_int] = field_type
                    except ValueError:
                        continue
                
                # í…Œì´ë¸” êµ¬ì¡° ì •ë³´ë¥¼ ë¡œê·¸ì— ê¸°ë¡
                logger.info(f"ë§¤í•‘ ê²°ê³¼ - ê°€ë¡œ: {column_mappings}")
                if result.row_mappings:
                    row_mappings = {}
                    for row_str, field_type in result.row_mappings.items():
                        try:
                            row_int = int(row_str)
                            row_mappings[row_int] = field_type
                        except ValueError:
                            continue
                    logger.info(f"ë§¤í•‘ ê²°ê³¼ - ì„¸ë¡œ: {row_mappings}")
                        
                return column_mappings
            else:
                logger.debug(f"AI í—¤ë” ë¶„ì„ ì‹ ë¢°ë„ ë‚®ìŒ: {result.confidence if result else 'N/A'}")
                
        except Exception as e:
            logger.debug(f"AI í—¤ë” ë¶„ì„ ì‹¤íŒ¨: {e}")
            
        return {}

    def _find_column_by_ai_mapping(self, ai_mappings: Dict[int, str], target_field_type: str) -> Optional[int]:
        """AI ë§¤í•‘ ê²°ê³¼ì—ì„œ íŠ¹ì • í•„ë“œ íƒ€ì…ì˜ ì—´ ë²ˆí˜¸ ì°¾ê¸°"""
        for col_idx, field_type in ai_mappings.items():
            if field_type == target_field_type:
                return col_idx
        return None

    async def _save_new_key(self, key_name: str, anchor_text: str, category: str = "special") -> None:
        """
        ìƒˆë¡œ ë°œê²¬ëœ í‚¤ë¥¼ íŒŒì¼ì— ì €ì¥
        
        Args:
            key_name: í‚¤ ì´ë¦„
            anchor_text: ë°œê²¬ëœ ì•µì»¤ í…ìŠ¤íŠ¸
            category: í‚¤ ì¹´í…Œê³ ë¦¬ (basic, special)
        """
        try:
            import json
            import os
            from datetime import datetime
            
            # ìƒˆ í‚¤ ì €ì¥ íŒŒì¼ ê²½ë¡œ
            new_keys_file = os.path.join(os.path.dirname(__file__), '..', 'data', 'new_keys_discovered.json')
            
            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            if os.path.exists(new_keys_file):
                with open(new_keys_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = {
                    "discovered_keys": [],
                    "last_updated": None,
                    "total_discovered": 0
                }
            
            # ìƒˆ í‚¤ ì •ë³´ ìƒì„±
            new_key_info = {
                "key_name": key_name,
                "anchor_text": anchor_text,
                "category": category,
                "discovered_at": datetime.now().isoformat(),
                "file_path": None  # ë‚˜ì¤‘ì— íŒŒì¼ ì •ë³´ ì¶”ê°€ ê°€ëŠ¥
            }
            
            # ì¤‘ë³µ í™•ì¸
            existing_keys = [k["key_name"] for k in data["discovered_keys"]]
            if key_name not in existing_keys:
                data["discovered_keys"].append(new_key_info)
                data["total_discovered"] = len(data["discovered_keys"])
                data["last_updated"] = datetime.now().isoformat()
                
                # íŒŒì¼ì— ì €ì¥
                with open(new_keys_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ìƒˆ í‚¤ ì €ì¥: {key_name} (ì•µì»¤: '{anchor_text}', ì¹´í…Œê³ ë¦¬: {category})")
            else:
                logger.info(f"í‚¤ '{key_name}'ëŠ” ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"ìƒˆ í‚¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def recognize_keys_with_ai(
        self, 
        file_path: str, 
        processor_type: str = "pdfplumber",
        save_debug: bool = False
    ) -> Dict[str, Any]:
        """
        í‚¤ ì¸ì‹ í›„ AIë¥¼ ì‚¬ìš©í•˜ì—¬ ê°’ì„ ìë™ ì¶”ì¶œ
        
        Args:
            file_path: PDF íŒŒì¼ ê²½ë¡œ
            processor_type: PDF ì²˜ë¦¬ê¸° íƒ€ì…
            save_debug: ë””ë²„ê¹…ìš© íŒŒì¼ ì €ì¥ ì—¬ë¶€
            
        Returns:
            Dict: í‚¤ ì¸ì‹ ë° AI ì¶”ì¶œ ê²°ê³¼
        """
        try:
            # 1ë‹¨ê³„: ê¸°ì¡´ í‚¤ ì¸ì‹ ì‹¤í–‰
            recognition_result = await self.recognize_keys(file_path, processor_type, save_debug)
            
            if not recognition_result.get('success') or not recognition_result.get('recognized_keys'):
                return recognition_result
            
            # 2ë‹¨ê³„: AI ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if not ai_extraction_service.is_available():
                logger.warning("AI ì„œë¹„ìŠ¤ê°€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í‚¤ ì¸ì‹ ê²°ê³¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return recognition_result
            
            # 3ë‹¨ê³„: í…Œì´ë¸” ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            tables = await self.extract_tables(file_path, processor_type)
            if not tables:
                logger.warning("í…Œì´ë¸” ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return recognition_result
            
            # 4ë‹¨ê³„: AI ì¶”ì¶œ ìš”ì²­ ìƒì„±
            ai_requests = []
            for key_data in recognition_result['recognized_keys']:
                # í•´ë‹¹ í‚¤ì˜ í…Œì´ë¸” ì°¾ê¸°
                target_table = None
                for table in tables:
                    if (table.page_number == key_data.get('anchor_cell', {}).get('page_number', 1) and
                        table.rows and len(table.rows) > key_data.get('anchor_cell', {}).get('row', 0)):
                        target_table = table
                        break
                
                if target_table:
                    ai_request = AIExtractionRequest(
                        key_name=key_data['key'],
                        key_label=key_data.get('key_label', key_data['key']),
                        anchor_cell=key_data['anchor_cell'],
                        table_data=target_table.rows,
                        page_number=target_table.page_number,
                        context=f"íŒŒì¼: {Path(file_path).name}"
                    )
                    ai_requests.append(ai_request)
            
            # 5ë‹¨ê³„: AI ì¶”ì¶œ ì‹¤í–‰
            if ai_requests:
                ai_results = await ai_extraction_service.batch_extract_values(ai_requests)
                
                # 6ë‹¨ê³„: ê²°ê³¼ í†µí•©
                enhanced_keys = []
                for key_data in recognition_result['recognized_keys']:
                    # í•´ë‹¹ í‚¤ì˜ AI ê²°ê³¼ ì°¾ê¸°
                    ai_result = None
                    for result in ai_results:
                        if result and result.key_name == key_data['key']:
                            ai_result = result
                            break
                    
                    if ai_result and ai_result.confidence >= 0.7:
                        # AI ì¶”ì¶œ ì„±ê³µ - ê°’ ì—…ë°ì´íŠ¸
                        enhanced_key = key_data.copy()
                        enhanced_key['ai_extracted_value'] = ai_result.extracted_value
                        enhanced_key['ai_confidence'] = ai_result.confidence
                        enhanced_key['ai_reasoning'] = ai_result.reasoning
                        enhanced_key['ai_processing_time'] = ai_result.processing_time
                        enhanced_key['is_ai_enhanced'] = True
                        
                        # suggested_position ì—…ë°ì´íŠ¸
                        if 'suggested_position' in enhanced_key:
                            enhanced_key['suggested_position'] = {
                                'row': ai_result.suggested_position['row'],
                                'col': ai_result.suggested_position['col']
                            }
                        
                        enhanced_keys.append(enhanced_key)
                        logger.info(f"AI ì¶”ì¶œ ì„±ê³µ: {key_data['key']} -> {ai_result.extracted_value} (ì‹ ë¢°ë„: {ai_result.confidence:.2f})")
                    else:
                        # AI ì¶”ì¶œ ì‹¤íŒ¨ - ê¸°ë³¸ ê²°ê³¼ ì‚¬ìš©
                        enhanced_key = key_data.copy()
                        enhanced_key['is_ai_enhanced'] = False
                        enhanced_keys.append(enhanced_key)
                        logger.warning(f"AI ì¶”ì¶œ ì‹¤íŒ¨: {key_data['key']} (ì‹ ë¢°ë„: {ai_result.confidence if ai_result else 0:.2f})")
                else:
                    enhanced_keys = recognition_result['recognized_keys']
                
                # 7ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë°˜í™˜
                final_result = recognition_result.copy()
                final_result['recognized_keys'] = enhanced_keys
                final_result['ai_enhanced'] = True
                final_result['ai_successful_extractions'] = len([k for k in enhanced_keys if k.get('is_ai_enhanced', False)])
                final_result['ai_total_attempts'] = len(ai_requests)
                
                logger.info(f"AI í–¥ìƒëœ í‚¤ ì¸ì‹ ì™„ë£Œ: {final_result['ai_successful_extractions']}/{final_result['ai_total_attempts']} ì„±ê³µ")
                return final_result
            
            else:
                logger.warning("AI ì¶”ì¶œ ìš”ì²­ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return recognition_result
                
        except Exception as e:
            logger.error(f"AI í–¥ìƒëœ í‚¤ ì¸ì‹ ì‹¤íŒ¨: {e}")
            # AI ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‚¤ ì¸ì‹ ê²°ê³¼ ë°˜í™˜
            return await self.recognize_keys(file_path, processor_type)

    async def _save_debug_data(self, file_path: str, processor_type: str, extraction_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ë””ë²„ê¹…ìš© ë°ì´í„° ì €ì¥"""
        try:
            import hashlib
            import json
            from datetime import datetime
            from pathlib import Path
            
            # ë””ë²„ê¹… ë””ë ‰í† ë¦¬ ìƒì„±
            debug_dir = Path("debug_extractions")
            debug_dir.mkdir(exist_ok=True)
            
            # íŒŒì¼ í•´ì‹œ ë° íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ê³ ìœ  í‚¤ ìƒì„±
            file_hash = hashlib.md5(file_path.encode()).hexdigest()[:8]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            debug_filename = f"debug_{file_hash}_{processor_type}_{timestamp}.json"
            debug_file_path = debug_dir / debug_filename
            
            # ì €ì¥í•  ë°ì´í„° êµ¬ì„±
            debug_data = {
                "metadata": {
                    "file_path": file_path,
                    "processor_type": processor_type,
                    "extraction_timestamp": datetime.now().isoformat(),
                    "file_hash": file_hash,
                    "debug_session_id": f"{file_hash}_{timestamp}"
                },
                "extraction_results": {
                    "recognized_keys": extraction_data.get("recognized_keys", []),
                    "pattern_suggestions": extraction_data.get("pattern_suggestions", []),
                    "new_key_suggestions": extraction_data.get("new_key_suggestions", []),
                    "statistics": {
                        "total_found": extraction_data.get("total_found", 0),
                        "pages_analyzed": extraction_data.get("pages_analyzed", 0),
                        "tables_analyzed": extraction_data.get("tables_analyzed", 0),
                        "suggestions_count": extraction_data.get("suggestions_count", 0),
                        "new_keys_count": extraction_data.get("new_keys_count", 0)
                    }
                },
                "raw_tables": []
            }
            
            # í…Œì´ë¸” ë°ì´í„° ìš”ì•½ (ì „ì²´ ì €ì¥í•˜ë©´ ë„ˆë¬´ í¬ë¯€ë¡œ)
            if "raw_tables" in extraction_data:
                for table in extraction_data["raw_tables"]:
                    table_summary = {
                        "page_number": table.page_number,
                        "rows_count": len(table.rows),
                        "cols_count": len(table.rows[0]) if table.rows else 0,
                        "first_row": table.rows[0] if table.rows else [],
                        "sample_cells": table.rows[:3] if len(table.rows) > 3 else table.rows  # ì²˜ìŒ 3í–‰ë§Œ
                    }
                    debug_data["raw_tables"].append(table_summary)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(debug_file_path, 'w', encoding='utf-8') as f:
                json.dump(debug_data, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"ğŸ› ë””ë²„ê¹… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {debug_file_path}")
            
            return {
                "file_path": str(debug_file_path),
                "file_size": debug_file_path.stat().st_size,
                "debug_session_id": debug_data["metadata"]["debug_session_id"],
                "saved_at": debug_data["metadata"]["extraction_timestamp"]
            }
            
        except Exception as e:
            logger.error(f"ë””ë²„ê¹… ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return None


# ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ íŒ©í† ë¦¬ í•¨ìˆ˜
def get_extraction_service() -> ExtractionService:
    """ì¶”ì¶œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return ExtractionService()