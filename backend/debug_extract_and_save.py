#!/usr/bin/env python3
"""
ë””ë²„ê¹…ìš© ì¶”ì¶œ ë° ì €ì¥ ìŠ¤í¬ë¦½íŠ¸

ìƒ˜í”Œ PDF íŒŒì¼ë“¤ì—ì„œ í…Œì´ë¸”ì„ ì¶”ì¶œí•˜ê³  ê²°ê³¼ë¥¼ JSON/Pickleë¡œ ì €ì¥í•˜ì—¬
ì‹œë®¬ë ˆì´ì…˜ ë° ë””ë²„ê¹…ì— í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import pickle
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë°±ì—”ë“œ ëª¨ë“ˆ import
import sys
sys.path.append(str(Path(__file__).parent))

from services.extraction_service import ExtractionService
from storage.cache_manager import CacheManager
from models.table_models import ExtractionResult
from models.analysis_models import ExtractionLibrary
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DebugExtractor:
    """ë””ë²„ê¹…ìš© ì¶”ì¶œê¸°"""
    
    def __init__(self):
        self.extraction_service = ExtractionService()
        self.cache_manager = CacheManager(
            cache_dir=Path("debug_cache"),
            max_age_hours=24*7  # ì¼ì£¼ì¼ ë³´ê´€
        )
        self.output_dir = Path("debug_output")
        self.output_dir.mkdir(exist_ok=True)
        
    def _generate_file_key(self, file_path: str, library: str) -> str:
        """íŒŒì¼ê³¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  í‚¤ ìƒì„±"""
        content = f"{file_path}_{library}_{datetime.now().strftime('%Y%m%d')}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    async def extract_and_save_sample(self, sample_name: str, library: str = "pdfplumber"):
        """ìƒ˜í”Œ íŒŒì¼ ì¶”ì¶œ ë° ì €ì¥"""
        try:
            # ìƒ˜í”Œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            sample_paths = {
                "sample1": "uploads/0001_250123162148(ë°°ì¹˜ì „ 1ì°¨_1ì°¨_8ì¥ì§œë¦¬)_ì°¨íŠ¸ë„˜ë²„ ìœ _ì†¡ìœ ì§„_4.pdf",
                "sample2": "uploads/í•©ë³¸_1.pdf",
                "original1": "../samples/0001_250123162148(ë°°ì¹˜ì „ 1ì°¨_1ì°¨_8ì¥ì§œë¦¬)_ì°¨íŠ¸ë„˜ë²„ ìœ _ì†¡ìœ ì§„.pdf",
                "original2": "../samples/í•©ë³¸.pdf"
            }
            
            if sample_name not in sample_paths:
                raise ValueError(f"ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ: {list(sample_paths.keys())}")
            
            file_path = sample_paths[sample_name]
            logger.info(f"ğŸ” ìƒ˜í”Œ ì¶”ì¶œ ì‹œì‘: {sample_name} ({library})")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            full_path = Path(file_path)
            if not full_path.exists():
                logger.error(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {full_path}")
                return False
            
            # ì¶”ì¶œ ì‹¤í–‰
            start_time = datetime.now()
            tables = await self.extraction_service.extract_tables(
                file_path=str(full_path),
                library=library
            )
            end_time = datetime.now()
            
            # ExtractionResult ìƒì„±
            extraction_result = ExtractionResult(
                file_path=str(full_path),
                file_name=full_path.name,
                file_id=sample_name,
                success=True,
                total_pages=max(table.page_number for table in tables) if tables else 0,
                total_tables=len(tables),
                tables=tables,
                extraction_library=ExtractionLibrary(library),
                processing_time=(end_time - start_time).total_seconds(),
                extracted_at=end_time
            )
            
            # ê²°ê³¼ ì €ì¥
            await self._save_results(sample_name, library, extraction_result, tables)
            
            logger.info(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(tables)}ê°œ í…Œì´ë¸”, {extraction_result.processing_time:.2f}ì´ˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    async def _save_results(
        self, 
        sample_name: str, 
        library: str, 
        extraction_result: ExtractionResult,
        tables: List
    ):
        """ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì €ì¥"""
        
        file_key = self._generate_file_key(sample_name, library)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ìºì‹œ ë§¤ë‹ˆì €ì— ì €ì¥
        cache_key = f"{sample_name}_{library}_{file_key}"
        success = await self.cache_manager.save_extraction_result(cache_key, extraction_result)
        if success:
            logger.info(f"ğŸ“¦ ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_key}")
        
        # 2. JSON í˜•íƒœë¡œ ì €ì¥ (ì½ê¸° ì‰¬ìš´ í˜•íƒœ)
        json_file = self.output_dir / f"{sample_name}_{library}_{timestamp}.json"
        json_data = {
            "metadata": {
                "sample_name": sample_name,
                "library": library,
                "file_key": file_key,
                "extracted_at": extraction_result.extracted_at.isoformat(),
                "processing_time": extraction_result.processing_time,
                "total_tables": extraction_result.total_tables,
                "total_pages": extraction_result.total_pages
            },
            "extraction_result": extraction_result.model_dump(),
            "tables_summary": [
                {
                    "page": table.page_number,
                    "rows": len(table.rows),
                    "cols": len(table.rows[0]) if table.rows else 0,
                    "data_preview": table.rows[:3] if table.rows else []  # ì²˜ìŒ 3ì¤„ë§Œ
                }
                for table in tables
            ]
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"ğŸ“ JSON ì €ì¥ ì™„ë£Œ: {json_file}")
        
        # 3. Pickle í˜•íƒœë¡œ ì €ì¥ (ì™„ì „í•œ ê°ì²´)
        pickle_file = self.output_dir / f"{sample_name}_{library}_{timestamp}.pkl"
        pickle_data = {
            "extraction_result": extraction_result,
            "tables": tables,
            "metadata": {
                "sample_name": sample_name,
                "library": library,
                "file_key": file_key,
                "saved_at": datetime.now()
            }
        }
        
        with open(pickle_file, 'wb') as f:
            pickle.dump(pickle_data, f)
        logger.info(f"ğŸ¥’ Pickle ì €ì¥ ì™„ë£Œ: {pickle_file}")
        
        # 4. ê°„ë‹¨í•œ í†µê³„ ìš”ì•½ ì €ì¥
        summary_file = self.output_dir / f"summary_{sample_name}_{library}.txt"
        summary_content = f"""
=== ì¶”ì¶œ ê²°ê³¼ ìš”ì•½ ===
ìƒ˜í”Œëª…: {sample_name}
ë¼ì´ë¸ŒëŸ¬ë¦¬: {library}
íŒŒì¼ í‚¤: {file_key}
ì¶”ì¶œ ì¼ì‹œ: {extraction_result.extracted_at}
ì²˜ë¦¬ ì‹œê°„: {extraction_result.processing_time:.2f}ì´ˆ
ì´ í˜ì´ì§€: {extraction_result.total_pages}
ì´ í…Œì´ë¸”: {extraction_result.total_tables}

=== í…Œì´ë¸”ë³„ ìƒì„¸ ===
"""
        
        for i, table in enumerate(tables):
            summary_content += f"""
í…Œì´ë¸” {i+1}:
  - í˜ì´ì§€: {table.page_number}
  - í¬ê¸°: {len(table.rows)}í–‰ x {len(table.rows[0]) if table.rows else 0}ì—´
  - ì²« ë²ˆì§¸ í–‰: {table.rows[0] if table.rows else 'N/A'}
"""
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        logger.info(f"ğŸ“Š ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_file}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    extractor = DebugExtractor()
    
    print("ğŸ”§ ë””ë²„ê¹…ìš© ìƒ˜í”Œ ì¶”ì¶œ ë„êµ¬")
    print("=" * 50)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ ëª©ë¡
    samples = ["sample1", "sample2", "original1", "original2"]
    libraries = ["pdfplumber", "camelot"]
    
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ:")
    for sample in samples:
        print(f"  - {sample}")
    
    print("\nğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:")
    for lib in libraries:
        print(f"  - {lib}")
    
    # ëª¨ë“  ì¡°í•© ì¶”ì¶œ (ì„ íƒì )
    extract_all = input("\nëª¨ë“  ì¡°í•©ì„ ì¶”ì¶œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
    
    if extract_all:
        for sample in samples:
            for library in libraries:
                print(f"\nğŸš€ {sample} + {library} ì¶”ì¶œ ì¤‘...")
                success = await extractor.extract_and_save_sample(sample, library)
                if success:
                    print(f"âœ… {sample} + {library} ì™„ë£Œ")
                else:
                    print(f"âŒ {sample} + {library} ì‹¤íŒ¨")
    else:
        # ê°œë³„ ì„ íƒ
        sample = input(f"\nìƒ˜í”Œ ì„ íƒ ({'/'.join(samples)}): ")
        library = input(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ({'/'.join(libraries)}): ")
        
        if sample in samples and library in libraries:
            print(f"\nğŸš€ {sample} + {library} ì¶”ì¶œ ì¤‘...")
            success = await extractor.extract_and_save_sample(sample, library)
            if success:
                print(f"âœ… ì¶”ì¶œ ì™„ë£Œ!")
            else:
                print(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨")
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    print(f"\nğŸ“ ê²°ê³¼ í™•ì¸: backend/debug_output/ í´ë”")
    print(f"ğŸ“¦ ìºì‹œ í™•ì¸: backend/debug_cache/ í´ë”")

if __name__ == "__main__":
    asyncio.run(main())
